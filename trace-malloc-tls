Use thread-local storage for tracing suppression.  b=376874

diff --git a/tools/trace-malloc/lib/nsTraceMalloc.c b/tools/trace-malloc/lib/nsTraceMalloc.c
--- a/tools/trace-malloc/lib/nsTraceMalloc.c
+++ b/tools/trace-malloc/lib/nsTraceMalloc.c
@@ -67,6 +67,7 @@
 #include "nsTraceMalloc.h"
 #include "nscore.h"
 #include "prinit.h"
+#include "prthread.h"
 
 #ifdef XP_WIN32
 #include "nsStackFrameWin.h"
@@ -299,12 +300,6 @@ static char      *sdlogname = NULL; /* f
 static char      *sdlogname = NULL; /* filename for shutdown leak log */
 
 /*
- * This counter suppresses tracing, in case any tracing code needs to malloc,
- * and it must be tested and manipulated only within tmmon.
- */
-static uint32 suppress_tracing = 0;
-
-/*
  * This enables/disables trace-malloc logging.
  *
  * It is separate from suppress_tracing so that we do not have to pay
@@ -324,6 +319,86 @@ static int tracing_enabled = 1;
         if (tmmon)                                                            \
             PR_ExitMonitor(tmmon);                                            \
     PR_END_MACRO
+
+/*
+ * Thread-local storage.
+ */
+typedef struct tm_thread tm_thread;
+struct tm_thread {
+    /*
+     * This counter suppresses tracing, in case any tracing code needs
+     * to malloc.
+     */
+    uint32 suppress_tracing;
+};
+
+static PRUintn tpIndex;
+static tm_thread main_thread; /* 0-initialization is correct */
+static tm_thread suppressed_thread; /* initialized below */
+
+static PRMonitor *tlsmon = NULL; /* protects initializingtls */
+static PRBool initializingtls = PR_FALSE;
+
+PR_STATIC_CALLBACK(void)
+free_tm_thread(void *priv)
+{
+    tm_thread *t = (tm_thread*) priv;
+
+    PR_ASSERT(t->suppress_tracing == 0);
+
+    if (t != &main_thread) {
+        t->suppress_tracing = 1;
+        __libc_free(t);
+    }
+}
+
+static tm_thread *
+get_tm_thread()
+{
+    tm_thread *t;
+
+    if (!tmmon) {
+        return &main_thread;
+    }
+
+    /* 
+     * We need to malloc here, but so do PR_SetThreadPrivate and
+     * PR_GetThreadPrivate.  (And we can't rely on __libc_malloc
+     * not calling our hooks on Windows.)
+     *
+     * So, to avoid infinite recursion during thread initialization,
+     * we need to protect our use of thread-local storage with a
+     * monitor.  So we don't get any of the performance benefits of
+     * thread-local storage, but at least we get the correct locking
+     * behavior.
+     */
+    PR_EnterMonitor(tlsmon);
+
+    if (initializingtls) {
+        /*
+         * This is a callback from the PR_GetThreadPrivate,
+         * __libc_malloc, or PR_SetThreadPrivate call below.  So return
+         * a tmthread that will be marked as suppressed.
+         */
+        t = &suppressed_thread;
+    } else {
+        initializingtls = PR_TRUE;
+
+        t = PR_GetThreadPrivate(tpIndex);
+
+        if (!t) {
+            t = (tm_thread*) __libc_malloc(sizeof(tm_thread));
+            t->suppress_tracing = 0;
+            PR_SetThreadPrivate(tpIndex, t);
+        }
+
+        initializingtls = PR_FALSE;
+    }
+
+    PR_ExitMonitor(tlsmon);
+
+    return t;
+}
 
 /* We don't want more than 32 logfiles open at once, ok? */
 typedef uint32          lfd_set;
@@ -1355,26 +1430,26 @@ static callsite *calltree(void **bp)
 #ifdef XP_WIN32
 
 callsite *
-backtrace(int skip)
+backtrace(int skip, tm_thread *t)
 {
     callsite *site;
 
     tmstats.backtrace_calls++;
-    suppress_tracing++;
+    t->suppress_tracing++;
 
     site = calltree(skip);
     if (!site) {
         tmstats.backtrace_failures++;
         /* PR_ASSERT(tmstats.backtrace_failures < 100); */
     }
-    suppress_tracing--;
+    t->suppress_tracing--;
     return site;
 }
 
 #else /*XP_UNIX*/
 
 callsite *
-backtrace(int skip)
+backtrace(int skip, tm_thread *t)
 {
     void **bp, **bpdown;
     callsite *site, **key;
@@ -1383,7 +1458,7 @@ backtrace(int skip)
     int i, n;
 
     tmstats.backtrace_calls++;
-    suppress_tracing++;
+    t->suppress_tracing++;
 
     /* Stack walking code adapted from Kipp's "leaky". */
 #if defined(__i386) 
@@ -1410,7 +1485,7 @@ backtrace(int skip)
         tmstats.backtrace_failures++;
         PR_ASSERT(tmstats.backtrace_failures < 100);
     }
-    suppress_tracing--;
+    t->suppress_tracing--;
     return site;
 }
 
@@ -1501,10 +1576,13 @@ malloc(size_t size)
     callsite *site;
     PLHashEntry *he;
     allocation *alloc;
+    tm_thread *t;
 
     if (!tracing_enabled || !PR_Initialized()) {
         return __libc_malloc(size);
     }
+
+    t = get_tm_thread();
 
     start = PR_IntervalNow();
     ptr = __libc_malloc(size);
@@ -1513,16 +1591,16 @@ malloc(size_t size)
     tmstats.malloc_calls++;
     if (!ptr) {
         tmstats.malloc_failures++;
-    } else if (suppress_tracing == 0) {
-        site = backtrace(1);
+    } else if (t->suppress_tracing == 0) {
+        site = backtrace(1, t);
         if (site)
             log_event5(logfp, TM_EVENT_MALLOC,
                        site->serial, start, end - start,
                        (uint32)NS_PTR_TO_INT32(ptr), size);
         if (get_allocations()) {
-            suppress_tracing++;
+            t->suppress_tracing++;
             he = PL_HashTableAdd(allocations, ptr, site);
-            suppress_tracing--;
+            t->suppress_tracing--;
             if (he) {
                 alloc = (allocation*) he;
                 alloc->size = size;
@@ -1542,6 +1620,7 @@ calloc(size_t count, size_t size)
     callsite *site;
     PLHashEntry *he;
     allocation *alloc;
+    tm_thread *t;
 
     /**
      * During the initialization of the glibc/libpthread, and
@@ -1558,6 +1637,8 @@ calloc(size_t count, size_t size)
         return __libc_calloc(count, size);
     }
 
+    t = get_tm_thread();
+
     start = PR_IntervalNow();
     ptr = __libc_calloc(count, size);
     end = PR_IntervalNow();
@@ -1565,8 +1646,8 @@ calloc(size_t count, size_t size)
     tmstats.calloc_calls++;
     if (!ptr) {
         tmstats.calloc_failures++;
-    } else if (suppress_tracing == 0) {
-        site = backtrace(1);
+    } else if (t->suppress_tracing == 0) {
+        site = backtrace(1, t);
         size *= count;
         if (site) {
             log_event5(logfp, TM_EVENT_CALLOC,
@@ -1574,9 +1655,9 @@ calloc(size_t count, size_t size)
                        (uint32)NS_PTR_TO_INT32(ptr), size);
         }
         if (get_allocations()) {
-            suppress_tracing++;
+            t->suppress_tracing++;
             he = PL_HashTableAdd(allocations, ptr, site);
-            suppress_tracing--;
+            t->suppress_tracing--;
             if (he) {
                 alloc = (allocation*) he;
                 alloc->size = size;
@@ -1599,14 +1680,17 @@ realloc(__ptr_t ptr, size_t size)
     PLHashEntry **hep, *he;
     allocation *alloc;
     FILE *trackfp = NULL;
+    tm_thread *t;
 
     if (!tracing_enabled || !PR_Initialized()) {
         return __libc_realloc(ptr, size);
     }
 
+    t = get_tm_thread();
+
     TM_ENTER_MONITOR();
     tmstats.realloc_calls++;
-    if (suppress_tracing == 0) {
+    if (t->suppress_tracing == 0) {
         oldptr = ptr;
         oldsite = NULL;
         oldsize = 0;
@@ -1643,8 +1727,8 @@ realloc(__ptr_t ptr, size_t size)
          * we'll leave the allocation entry untouched.
          */
         tmstats.realloc_failures++;
-    } else if (suppress_tracing == 0) {
-        site = backtrace(1);
+    } else if (t->suppress_tracing == 0) {
+        site = backtrace(1, t);
         if (site) {
             log_event8(logfp, TM_EVENT_REALLOC,
                        site->serial, start, end - start,
@@ -1653,7 +1737,7 @@ realloc(__ptr_t ptr, size_t size)
                        (uint32)NS_PTR_TO_INT32(oldptr), oldsize);
         }
         if (ptr && allocations) {
-            suppress_tracing++;
+            t->suppress_tracing++;
             if (ptr != oldptr) {
                 /*
                  * If we're reallocating (not merely allocating new space by
@@ -1673,7 +1757,7 @@ realloc(__ptr_t ptr, size_t size)
                 if (!he)
                     he = PL_HashTableAdd(allocations, ptr, site);
             }
-            suppress_tracing--;
+            t->suppress_tracing--;
             if (he) {
                 alloc = (allocation*) he;
                 alloc->size = size;
@@ -1693,10 +1777,13 @@ valloc(size_t size)
     callsite *site;
     PLHashEntry *he;
     allocation *alloc;
+    tm_thread *t;
 
     if (!tracing_enabled || !PR_Initialized()) {
         return __libc_valloc(size);
     }
+
+    t = get_tm_thread();
 
     start = PR_IntervalNow();
     ptr = __libc_valloc(size);
@@ -1705,16 +1792,16 @@ valloc(size_t size)
     tmstats.malloc_calls++; /* XXX valloc_calls ? */
     if (!ptr) {
         tmstats.malloc_failures++; /* XXX valloc_failures ? */
-    } else if (suppress_tracing == 0) {
-        site = backtrace(1);
+    } else if (t->suppress_tracing == 0) {
+        site = backtrace(1, t);
         if (site)
             log_event5(logfp, TM_EVENT_MALLOC, /* XXX TM_EVENT_VALLOC? */
                        site->serial, start, end - start,
                        (uint32)NS_PTR_TO_INT32(ptr), size);
         if (get_allocations()) {
-            suppress_tracing++;
+            t->suppress_tracing++;
             he = PL_HashTableAdd(allocations, ptr, site);
-            suppress_tracing--;
+            t->suppress_tracing--;
             if (he) {
                 alloc = (allocation*) he;
                 alloc->size = size;
@@ -1734,10 +1821,13 @@ memalign(size_t boundary, size_t size)
     callsite *site;
     PLHashEntry *he;
     allocation *alloc;
+    tm_thread *t;
 
     if (!tracing_enabled || !PR_Initialized()) {
         return __libc_memalign(boundary, size);
     }
+
+    t = get_tm_thread();
 
     start = PR_IntervalNow();
     ptr = __libc_memalign(boundary, size);
@@ -1746,17 +1836,17 @@ memalign(size_t boundary, size_t size)
     tmstats.malloc_calls++; /* XXX memalign_calls ? */
     if (!ptr) {
         tmstats.malloc_failures++; /* XXX memalign_failures ? */
-    } else if (suppress_tracing == 0) {
-        site = backtrace(1);
+    } else if (t->suppress_tracing == 0) {
+        site = backtrace(1, t);
         if (site) {
             log_event5(logfp, TM_EVENT_MALLOC, /* XXX TM_EVENT_MEMALIGN? */
                        site->serial, start, end - start,
                        (uint32)NS_PTR_TO_INT32(ptr), size);
         }
         if (get_allocations()) {
-            suppress_tracing++;
+            t->suppress_tracing++;
             he = PL_HashTableAdd(allocations, ptr, site);
-            suppress_tracing--;
+            t->suppress_tracing--;
             if (he) {
                 alloc = (allocation*) he;
                 alloc->size = size;
@@ -1786,17 +1876,20 @@ free(__ptr_t ptr)
     allocation *alloc;
     uint32 serial = 0, size = 0;
     PRUint32 start, end;
+    tm_thread *t;
 
     if (!tracing_enabled || !PR_Initialized()) {
         __libc_free(ptr);
         return;
     }
+
+    t = get_tm_thread();
 
     TM_ENTER_MONITOR();
     tmstats.free_calls++;
     if (!ptr) {
         tmstats.null_free_calls++;
-    } else if (suppress_tracing == 0) {
+    } else if (t->suppress_tracing == 0) {
         if (get_allocations()) {
             hep = PL_HashTableRawLookup(allocations, hash_pointer(ptr), ptr);
             he = *hep;
@@ -1852,7 +1945,6 @@ PR_IMPLEMENT(void) NS_TraceMallocStartup
 PR_IMPLEMENT(void) NS_TraceMallocStartup(int logfd)
 {
     /* We must be running on the primordial thread. */
-    PR_ASSERT(suppress_tracing == 0);
     PR_ASSERT(tracing_enabled == 1);
     PR_ASSERT(logfp == &default_logfile);
     tracing_enabled = (logfd >= 0);
@@ -1869,7 +1961,20 @@ PR_IMPLEMENT(void) NS_TraceMallocStartup
     }
 
     atexit(NS_TraceMallocShutdown);
+
+    /*
+     * We only allow one thread until NS_TraceMallocStartup is called.
+     * When it is, we have to initialize tpIndex before allocating tmmon
+     * since get_tm_index uses NULL-tmmon to detect tpIndex being
+     * uninitialized.
+     */
+    main_thread.suppress_tracing++;
+    suppressed_thread.suppress_tracing = 1;
+    tlsmon = PR_NewMonitor();
+    PR_NewThreadPrivateIndex(&tpIndex, free_tm_thread);
+    PR_SetThreadPrivate(tpIndex, &main_thread);
     tmmon = PR_NewMonitor();
+    main_thread.suppress_tracing--;
 
 #ifdef XP_WIN32
     /* Register listeners for win32. */
@@ -2055,6 +2160,10 @@ PR_IMPLEMENT(void) NS_TraceMallocShutdow
         tmmon = NULL;
         PR_DestroyMonitor(mon);
     }
+    if (tlsmon) {
+        PR_DestroyMonitor(tlsmon);
+        tlsmon = NULL;
+    }
 #ifdef XP_WIN32
     if (tracing_enabled) {
         ShutdownHooker();
@@ -2226,8 +2335,9 @@ NS_TraceStack(int skip, FILE *ofp)
 NS_TraceStack(int skip, FILE *ofp)
 {
     callsite *site;
-
-    site = backtrace(skip + 1);
+    tm_thread *t = get_tm_thread();
+
+    site = backtrace(skip + 1, t);
     while (site) {
         if (site->name || site->parent) {
             fprintf(ofp, "%s[%s +0x%X]\n",
@@ -2296,24 +2406,27 @@ MallocCallback(void *ptr, size_t size, P
     callsite *site;
     PLHashEntry *he;
     allocation *alloc;
+    tm_thread *t;
 
     if (!tracing_enabled)
         return;
+
+    t = get_tm_thread();
 
     TM_ENTER_MONITOR();
     tmstats.malloc_calls++;
     if (!ptr) {
         tmstats.malloc_failures++;
-    } else if (suppress_tracing == 0) {
-        site = backtrace(4);
+    } else if (t->suppress_tracing == 0) {
+        site = backtrace(4, t);
         if (site)
             log_event5(logfp, TM_EVENT_MALLOC,
                        site->serial, start, end - start,
                        (uint32)NS_PTR_TO_INT32(ptr), size);
         if (get_allocations()) {
-            suppress_tracing++;
+            t->suppress_tracing++;
             he = PL_HashTableAdd(allocations, ptr, site);
-            suppress_tracing--;
+            t->suppress_tracing--;
             if (he) {
                 alloc = (allocation*) he;
                 alloc->size = size;
@@ -2329,25 +2442,28 @@ CallocCallback(void *ptr, size_t count, 
     callsite *site;
     PLHashEntry *he;
     allocation *alloc;
+    tm_thread *t;
 
     if (!tracing_enabled)
         return;
+
+    t = get_tm_thread();
 
     TM_ENTER_MONITOR();
     tmstats.calloc_calls++;
     if (!ptr) {
         tmstats.calloc_failures++;
-    } else if (suppress_tracing == 0) {
-        site = backtrace(1);
+    } else if (t->suppress_tracing == 0) {
+        site = backtrace(1, t);
         size *= count;
         if (site)
             log_event5(logfp, TM_EVENT_CALLOC,
                        site->serial, start, end - start,
                        (uint32)NS_PTR_TO_INT32(ptr), size);
         if (get_allocations()) {
-            suppress_tracing++;
+            t->suppress_tracing++;
             he = PL_HashTableAdd(allocations, ptr, site);
-            suppress_tracing--;
+            t->suppress_tracing--;
             if (he) {
                 alloc = (allocation*) he;
                 alloc->size = size;
@@ -2365,13 +2481,16 @@ ReallocCallback(void * oldptr, void *ptr
     PLHashNumber hash;
     PLHashEntry **hep, *he;
     allocation *alloc;
+    tm_thread *t;
 
     if (!tracing_enabled)
         return;
 
+    t = get_tm_thread();
+
     TM_ENTER_MONITOR();
     tmstats.realloc_calls++;
-    if (suppress_tracing == 0) {
+    if (t->suppress_tracing == 0) {
         oldsite = NULL;
         oldsize = 0;
         he = NULL;
@@ -2393,8 +2512,8 @@ ReallocCallback(void * oldptr, void *ptr
          * When realloc() fails, the original block is not freed or moved, so
          * we'll leave the allocation entry untouched.
          */
-    } else if (suppress_tracing == 0) {
-        site = backtrace(1);
+    } else if (t->suppress_tracing == 0) {
+        site = backtrace(1, t);
         if (site) {
             log_event8(logfp, TM_EVENT_REALLOC,
                        site->serial, start, end - start,
@@ -2403,7 +2522,7 @@ ReallocCallback(void * oldptr, void *ptr
                        (uint32)NS_PTR_TO_INT32(oldptr), oldsize);
         }
         if (ptr && allocations) {
-            suppress_tracing++;
+            t->suppress_tracing++;
             if (ptr != oldptr) {
                 /*
                  * If we're reallocating (not allocating new space by passing
@@ -2422,7 +2541,7 @@ ReallocCallback(void * oldptr, void *ptr
                 if (!he)
                     he = PL_HashTableAdd(allocations, ptr, site);
             }
-            suppress_tracing--;
+            t->suppress_tracing--;
             if (he) {
                 alloc = (allocation*) he;
                 alloc->size = size;
@@ -2438,15 +2557,18 @@ FreeCallback(void * ptr, PRUint32 start,
     PLHashEntry **hep, *he;
     callsite *site;
     allocation *alloc;
+    tm_thread *t;
 
     if (!tracing_enabled)
         return;
+
+    t = get_tm_thread();
 
     TM_ENTER_MONITOR();
     tmstats.free_calls++;
     if (!ptr) {
         tmstats.null_free_calls++;
-    } else if (suppress_tracing == 0) {
+    } else if (t->suppress_tracing == 0) {
         if (get_allocations()) {
             hep = PL_HashTableRawLookup(allocations, hash_pointer(ptr), ptr);
             he = *hep;
